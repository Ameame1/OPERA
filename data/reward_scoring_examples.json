{
  "metadata": {
    "type": "reward_scoring_examples",
    "description": "Examples showing how rewards are calculated for different agent outputs"
  },
  "scoring_examples": {
    "plan_agent_scoring": [
      {
        "example_id": "plan_score_001",
        "input_question": "What is the currency of the country where the inventor of dynamite was born?",
        "candidate_output": [
          {
            "subgoal_id": 1,
            "subgoal": "Who invented dynamite?",
            "dependencies": []
          },
          {
            "subgoal_id": 2,
            "subgoal": "Where was [entity from step 1] born?",
            "dependencies": [
              1
            ]
          },
          {
            "subgoal_id": 3,
            "subgoal": "What country is [entity from step 2] in?",
            "dependencies": [
              2
            ]
          },
          {
            "subgoal_id": 4,
            "subgoal": "What is the currency of [entity from step 3]?",
            "dependencies": [
              3
            ]
          }
        ],
        "scoring_breakdown": {
          "logic_score": 0.95,
          "logic_details": "Perfect logical flow with correct dependencies",
          "execution_score": 0.9,
          "execution_details": "All steps are executable with standard retrieval",
          "accuracy_score": 1.0,
          "accuracy_details": "Decomposition correctly identifies all needed information",
          "efficiency_score": 0.85,
          "efficiency_details": "4 steps is reasonable but could potentially be 3",
          "placeholder_score": 1.0,
          "placeholder_details": "Correct use of [entity from step X] throughout"
        },
        "final_score": 0.94,
        "calculation": "(0.25*0.95 + 0.25*0.90 + 0.30*1.0 + 0.10*0.85 + 0.10*1.0)"
      },
      {
        "example_id": "plan_score_002",
        "input_question": "Who is the current mayor of the city where the Olympics were held in 2012?",
        "candidate_output": [
          {
            "subgoal_id": 1,
            "subgoal": "Where were the Olympics held in 2012?",
            "dependencies": []
          },
          {
            "subgoal_id": 2,
            "subgoal": "Who is the current mayor of London?",
            "dependencies": [
              1
            ]
          }
        ],
        "scoring_breakdown": {
          "logic_score": 0.7,
          "logic_details": "Step 2 hardcodes 'London' instead of using placeholder",
          "execution_score": 0.8,
          "execution_details": "Steps are executable but breaks if step 1 fails",
          "accuracy_score": 0.6,
          "accuracy_details": "Assumes answer without proper dependency",
          "efficiency_score": 1.0,
          "efficiency_details": "Minimal steps needed",
          "placeholder_score": 0.0,
          "placeholder_details": "Failed to use placeholder mechanism"
        },
        "final_score": 0.65,
        "calculation": "(0.25*0.7 + 0.25*0.8 + 0.30*0.6 + 0.10*1.0 + 0.10*0.0)"
      }
    ],
    "analysis_agent_scoring": [
      {
        "example_id": "analysis_score_001",
        "sub_question": "Who founded Tesla Motors?",
        "documents": [
          "Tesla was founded in 2003 by Martin Eberhard and Marc Tarpenning..."
        ],
        "candidate_output": {
          "status": "yes",
          "answer": "Martin Eberhard and Marc Tarpenning",
          "analysis": "The document clearly states Tesla was founded in 2003 by Martin Eberhard and Marc Tarpenning. This directly answers the question about Tesla's founders."
        },
        "scoring_breakdown": {
          "accuracy_score": 1.0,
          "accuracy_details": "Correctly extracted founder information",
          "completeness_score": 0.9,
          "completeness_details": "Mentioned both founders but not Elon Musk's later role",
          "reasoning_score": 0.85,
          "reasoning_details": "Clear reasoning but could be more comprehensive",
          "confidence_score": 1.0,
          "confidence_details": "Appropriate confidence for direct information"
        },
        "final_score": 0.92,
        "calculation": "(0.40*1.0 + 0.20*0.9 + 0.30*0.85 + 0.10*1.0)"
      },
      {
        "example_id": "analysis_score_002",
        "sub_question": "What is the population of Mars?",
        "documents": [
          "Mars is the fourth planet from the Sun with a diameter of 6,779 km..."
        ],
        "candidate_output": {
          "status": "yes",
          "answer": "0",
          "analysis": "Mars has no population as it's an uninhabited planet."
        },
        "scoring_breakdown": {
          "accuracy_score": 0.3,
          "accuracy_details": "Misunderstood question context",
          "completeness_score": 0.2,
          "completeness_details": "Failed to recognize information gap",
          "reasoning_score": 0.4,
          "reasoning_details": "Poor reasoning about what documents contain",
          "confidence_score": 0.0,
          "confidence_details": "Overconfident about unavailable information"
        },
        "final_score": 0.28,
        "calculation": "(0.40*0.3 + 0.20*0.2 + 0.30*0.4 + 0.10*0.0)"
      }
    ],
    "rewrite_agent_scoring": [
      {
        "example_id": "rewrite_score_001",
        "original_query": "What is the GDP of [entity from step 2]?",
        "failure_reason": "Documents lack specific GDP statistics for the country",
        "candidate_output": {
          "rewritten_query": "GDP gross domestic product economy economic data statistics [entity from step 2] trillion billion economic indicators financial data",
          "strategy": "economic_expansion",
          "keywords": [
            "GDP",
            "economy",
            "statistics",
            "economic indicators"
          ]
        },
        "scoring_breakdown": {
          "relevance_score": 0.95,
          "relevance_details": "Highly relevant economic terms added",
          "diversity_score": 0.9,
          "diversity_details": "Good variety of economic terminology",
          "specificity_score": 0.85,
          "specificity_details": "Maintains focus while expanding vocabulary",
          "strategy_score": 1.0,
          "strategy_details": "Appropriate strategy for statistical query"
        },
        "final_score": 0.91,
        "calculation": "(0.30*0.95 + 0.25*0.90 + 0.25*0.85 + 0.20*1.0)"
      },
      {
        "example_id": "rewrite_score_002",
        "original_query": "Who is the CEO of Microsoft?",
        "failure_reason": "Current information not in retrieved documents",
        "candidate_output": {
          "rewritten_query": "Microsoft CEO chief executive officer Satya Nadella leadership management corporate",
          "strategy": "direct_injection",
          "keywords": [
            "CEO",
            "Satya Nadella",
            "leadership"
          ]
        },
        "scoring_breakdown": {
          "relevance_score": 0.6,
          "relevance_details": "Includes answer in query (Satya Nadella)",
          "diversity_score": 0.7,
          "diversity_details": "Limited expansion beyond direct terms",
          "specificity_score": 0.5,
          "specificity_details": "Too specific with name injection",
          "strategy_score": 0.3,
          "strategy_details": "Poor strategy - shouldn't inject potential answers"
        },
        "final_score": 0.54,
        "calculation": "(0.30*0.6 + 0.25*0.7 + 0.25*0.5 + 0.20*0.3)"
      }
    ]
  },
  "scoring_guidelines": {
    "plan_agent": {
      "weights": {
        "logic": 0.25,
        "execution": 0.25,
        "accuracy": 0.3,
        "efficiency": 0.1,
        "placeholder": 0.1
      },
      "high_reward_patterns": [
        "Correct dependency chains",
        "Proper placeholder usage",
        "Efficient decomposition"
      ],
      "low_reward_patterns": [
        "Hardcoded values",
        "Missing dependencies",
        "Over-decomposition"
      ]
    },
    "analysis_agent": {
      "weights": {
        "accuracy": 0.4,
        "completeness": 0.2,
        "reasoning": 0.3,
        "confidence": 0.1
      },
      "high_reward_patterns": [
        "Accurate extraction",
        "Comprehensive analysis",
        "Appropriate confidence"
      ],
      "low_reward_patterns": [
        "Hallucination",
        "Missing information",
        "Overconfidence"
      ]
    },
    "rewrite_agent": {
      "weights": {
        "relevance": 0.3,
        "diversity": 0.25,
        "specificity": 0.25,
        "strategy": 0.2
      },
      "high_reward_patterns": [
        "Context-aware expansion",
        "Diverse vocabulary",
        "Appropriate strategy"
      ],
      "low_reward_patterns": [
        "Answer injection",
        "Over-generalization",
        "Poor strategy choice"
      ]
    }
  }
}